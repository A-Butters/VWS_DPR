---
title: "Quantitative analysis for DPR Working Paper"
author: "Anna Butters"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
knitr:
  opts_chunk:
    out.width: 90%
    fig.width: 10
    fig.align: center
editor_options: 
  markdown: 
    wrap: 72
---

R 4.4.1 "Race for Your Life"

```{r}
## Uncomment to install packages if not already installed
#install.packages("tidyverse")
#install.packages("glue")
#install.packages("car")
#install.packages("lmtest")
#install.packages("sjPlot")
#install.packages("irr")
#install.packages("jtools")
#install.packages("vtable")
#install.packages("modelsummary")
#install.packages("survival")
#install.packages("gtsummary")
#install.packages("readxl")
#install.packages("estimatr")
#install.packages("FSA")
#install.packages("lme4")
#install.packages("merTools")
#install.packages("performance")

## Load packages
library(tidyverse)
library(glue)
library(car)
library(lmtest)
library(sjPlot)
library(irr)
library(jtools)
library(vtable)
library(modelsummary)
library(survival)
library(gtsummary)
library(readxl)
library(estimatr)
library(FSA)
library(lme4)
library(merTools)
library(performance)

## Get the data 
internal <- read_csv("raw_data/internal_selection_shortlist_2024-10-02.csv") # Shortlisting for panel
panel_qa<- read_csv("raw_data/panel_quick_assessments_2024-10-02.csv") # Panel quick assessments
panel_meeting <- read_csv("raw_data/panel_meeting_funding_2024-10-02.csv") # Panel meeting outcome
d5 <- read_csv("raw_data/review_data_pseudonomized_2024-08-23.csv") # DPR review data
load("raw_data/review_disciplines_2024-10-03.Rdata") # Review disciplines (d9)
rc_1_exp <- read_csv("raw_data/reviewer_characteristics_2024-11-12.csv") # reviewer characteristics
feedback_df <- read_excel("raw_data/feedback_pseudo.xlsx") # Feedback survey
expect_df <- read_excel("raw_data/data_consent_form_pseudo_2025-02-14.xlsx") # Expectations survey
sent <- read_csv("data_merged/comments_sentiment_classification_2024-11-05.csv") # Sentiment analysis

# Set theme for all plots
theme_set(theme_bw() +
  theme(plot.title = element_text(size = 12), 
        axis.title = element_text(size = 11),
        axis.text = element_text(size = 10),
        legend.position = "bottom",
        legend.text = element_text(size = 11)))
```

## Checking the data

```{r}
## Panel data
glimpse(internal)
glimpse(panel_qa)
glimpse(panel_meeting)

##  DPR data
glimpse(d5)
glimpse(d9)
glimpse(rc_1_exp)
glimpse(feedback_df)
glimpse(expect_df)

## Check all response_ids are unique (reviewer_id for rc_1_exp)
anyDuplicated(d5$response_id) # 0
anyDuplicated(panel_qa$response_id) # 0
anyDuplicated(rc_1_exp$reviewer_id) #0
## Check min and max of all dpr variables  
summary(d5)

## Join panel data
df1 <-left_join(internal, panel_qa, by = "proposal_id")
prp <- left_join(df1, panel_meeting, by = "proposal_id")

## Join dpr data
df2 <- full_join(d5, d9, by = c("response_id", "proposal_id",
                                "reviewer_id"))
dpr <- left_join(df2, rc_1_exp, by = "reviewer_id")

```

## Cleaning the data

-   Adding variables
-   Recoding

```{r}
## Recode scores to change letters to numbers 
dpr <- dpr %>% # For DPR
    mutate(across(c(crit1:others_avg), 
                ~ dplyr::recode(., 'A+' = 9, 'A' = 8, 'A-' = 7, 
                         'B+' = 6, 'B' = 5, 'B-' = 4, 
                         'C+' = 3, 'C' = 2, 'C-' = 1)))

prp <- prp %>% # For panel
    mutate(across(c(qa_rev1:vote.x, crit1:others_avg), 
                ~ dplyr::recode(., 'A+' = 9, 'A' = 8, 'A-' = 7, 
                         'B+' = 6, 'B' = 5, 'B-' = 4, 
                         'C+' = 3, 'C' = 2, 'C-' = 1, 'A-B' = 7))) # 
# Check qa combined ratings
table(prp$qa_combined) 

# Create variable for uncertainty and numeric variable for disciplinary fit
dpr <- dpr %>%
    #Adjust values of people who put highest/lowest confidence in wrong order
  mutate(conf_highest2 = conf_highest, #Use duplicates for comparisons to avoid error after conf_highest changes
         conf_lowest2 = conf_lowest,
         conf_highest = if_else(conf_highest < conf_lowest, conf_lowest, conf_highest),
         conf_lowest = if_else(conf_highest2 < conf_lowest2, conf_highest2, conf_lowest2),
         #Calculate uncertainty
         uncertainty = conf_highest - conf_lowest) %>%
    ## Group by reviewer to get average uncertainty per reviewer
  group_by(reviewer_id) %>%
  mutate(avg_uncertainty = mean(uncertainty)) %>%
  ungroup() %>%
  ## Group by proposal to get average uncertainty per proposal
  group_by(proposal_id) %>%
  mutate(proposal_avg_uncertainty = mean(uncertainty)) %>%
  ungroup() %>%
    ## Create numeric variable for disciplinary fit
  mutate(numeric_dFit = case_when(
      disciplinary_fit == 
        "Certain that I was inappropriate to provide a review" ~ 1,
      disciplinary_fit == "Not confident" ~ 2,
      disciplinary_fit == "Low confidence" ~ 3,
      disciplinary_fit == "Somewhat confident" ~ 4,
      disciplinary_fit == "Confident" ~ 5,
      disciplinary_fit == "Very confident" ~ 6,
      disciplinary_fit == "Totally confident" ~ 7))

## Repeat for panel data
prp<- prp %>%
  #Adjust values of people who put highest/lowest confidence in wrong order
  mutate(conf_highest2 = conf_highest, #Use duplicates for comparisons to avoid error after conf_highest changes
         conf_lowest2 = conf_lowest,
         conf_highest = if_else(conf_highest < conf_lowest, conf_lowest, conf_highest),
         conf_lowest = if_else(conf_highest2 < conf_lowest2, conf_highest2, conf_lowest2),
         #Calculate confidence range
         uncertainty = conf_highest - conf_lowest) %>%
    ## Create numeric variable for disciplinary fit
  mutate(numeric_dFit = case_when(
      disciplinary_fit == 
        "Certain that I was inappropriate to provide a review" ~ 1,
      disciplinary_fit == "Not confident" ~ 2,
      disciplinary_fit == "Low confidence" ~ 3,
      disciplinary_fit == "Somewhat confident" ~ 4,
      disciplinary_fit == "Confident" ~ 5,
      disciplinary_fit == "Very confident" ~ 6,
      disciplinary_fit == "Totally confident" ~ 7))

```

```{r}
## Recode incorrectly specified time(hours not mins) based on highest time recorded 
table(dpr$review_time) #480 so anything below 8
dpr <- dpr %>%
  mutate(review_time = case_when(
    review_time < 8.0 ~ review_time *60, # multiply by 60 to convert to mins
    TRUE ~ review_time
  ))

## Create new variable 'matching' to show whether reviewer was classed as within 
## or outside of discipline during DPR reviewer:proposal matching
dpr <- dpr %>%
  rowwise() %>%
  mutate(matching = case_when(rev_d1 == pro_d1 ~ 1, ## Within discipline
                              rev_d1 != pro_d1 ~ 0, ## Outside of discipline
                              TRUE ~ NA))

## Create new variables for degree of review board overlap (0-6) between reviewer and proposal - proportion of the proposal disciplines also reported by reviewer
dpr <- dpr %>%
  rowwise() %>%
  mutate(overlap = sum(rev_dds %in% pro_dds),# Check no. of overlap between reviewer and proposal
         length_dds = length(pro_dds), # Check how many disciplines were reported for proposal
         overlap_prop = round(overlap/ length(pro_dds) *100, 2)) %>% # Get proportion of overlap 
  ungroup() %>%
  ## Add variable categorising overlap (none to Very high)
  mutate(overlap_scale = 
           case_when(overlap_prop == 0 ~ "None",                      
                     overlap_prop >0 & overlap_prop <= quantile(overlap_prop, 0.25) ~ "Low",
                     overlap_prop > quantile(overlap_prop, 0.25) & overlap_prop <= quantile(overlap_prop, 0.5) ~ "Moderate",
                     overlap_prop > quantile(overlap_prop, 0.5) & overlap_prop <100 ~ "High",
                     overlap_prop >=100 ~ "Very high"),
         overlap_scale = factor(overlap_scale, levels = c("None", "Low", "Moderate", "High", "Very high")))
table(dpr$overlap_scale)

## Create new variable for ECR status
dpr <- dpr %>%
  mutate(ecr = case_when(working_yrs <=5 ~ 1,
                         working_yrs >5 ~ 0,
                         TRUE ~ NA))
##Rename review boards 
dpr <- dpr %>%
  mutate(across(c(rev_d1:rev_d3), 
                ~ dplyr::recode(., '2' = "Life Sciences", '3' = "Natural Sciences", 
                                '101' = "Ancient Cultures", 
                         '102' = "History", '103' = "Art history, music, theeatre and media studies",
                         '104' = "Linguistics", '105' = "Literary Studies", '106' = "Social and cultural anthropology etc", '107'= "Theology", '108' = "Philosophy", '109' = "Educational Research", 
                         '110' = "Psychology", '111' = "Social Sciences", '112' = "Economics",
                         '113' = "Jurisprudence", '317' ="Geography", '409' = "Computer Science",
                         '410' = "Construction, Engineering and Architecture"))) %>%
  mutate(across(c(pro_d1:pro_d3), 
                ~ dplyr::recode(., '2' = "Life Sciences", '3' = "Natural Sciences", 
                                '101' = "Ancient Cultures", 
                         '102' = "History", '103' = "Art history, music, theeatre and media studies",
                         '104' = "Linguistics", '105' = "Literary Studies", '106' = "Social and cultural anthropology etc", '107'= "Theology", '108' = "Philosophy", '109' = "Educational Research", 
                         '110' = "Psychology", '111' = "Social Sciences", '112' = "Economics",
                         '113' = "Jurisprudence", '317' ="Geography", '409' = "Computer Science",
                         '410' = "Construction, Engineering and Architecture")))


```

```{r}
## Create factor variable for working years quantile
dpr<-dpr %>%
  mutate(working_quant = case_when(working_yrs <= quantile(working_yrs, 0.25, na.rm = T) ~ 1,
                                   working_yrs >quantile(working_yrs, 0.25, na.rm = T) &
                                   working_yrs <= quantile(working_yrs, 0.5, na.rm = T) ~ 2,
                                   working_yrs >quantile(working_yrs, 0.5, na.rm = T) &
                                   working_yrs <= quantile(working_yrs, 0.75, na.rm = T) ~ 3,
                                   working_yrs >quantile(working_yrs, 0.75, na.rm = T) &
                                   working_yrs <= quantile(working_yrs, 1, na.rm = T) ~ 4,
                                   TRUE ~ NA),
         working_quant = as.factor(working_quant))

## Get applicant characteristics i.e. join reviewer characteristics to their own proposals - pivot longer then wider to get unique names
dpr_char<- dpr %>%
  dplyr::select(reviewer_id, gender, first_gen, rev_d1, rev_d2, rev_d3, ecr, working_quant) %>%
  mutate(reviewer_no = sub(".*_", "", reviewer_id),
         proposal_id = sub("*_.", "", reviewer_id),
         ecr = as.factor(ecr)) %>%  # create proposal_id column for joining back later (this is the proposal the reviewer submitted rather than the ones they reviewed)
  pivot_longer(
    cols = c(gender, first_gen, rev_d1, rev_d2, rev_d3, ecr, working_quant),  # Convert to long format first
    names_to = "variable",  
    values_to = "value"     
  ) %>%
  pivot_wider(
    names_from = c(variable, reviewer_no),  
    names_sep = "_",
    values_from = value,
    values_fn = first # resolve duplicates (all identical)
  ) 

# Coalesce the data so all co-applicants for one proposal are on one row
dpr_char2 <- dpr_char %>%
  dplyr::select(-reviewer_id) %>%
  filter(!if_all(gender_2:working_quant_3, is.na)) %>% # Because some only had 2 applicants/reviewers
  group_by(proposal_id) %>%
  summarise(across(gender_2:working_quant_3, ~ first(na.omit(.))), .groups = "drop")  # Take the first non-NA value per column

## Join proposal applicants characteristics to dpr
dpr_char2$proposal_id <- as.integer(dpr_char2$proposal_id)

dpr <- left_join(dpr, dpr_char2[, c("proposal_id", "gender_1", "gender_2", "gender_3", "first_gen_1", "first_gen_2", "first_gen_3", "rev_d1_1", "rev_d1_2", "rev_d1_3", "rev_d2_1", "rev_d2_2", "rev_d2_3", "rev_d3_1", "rev_d3_2", "rev_d3_3",  "ecr_1", "ecr_2", "ecr_3",
                                    "working_quant_1", "working_quant_2", "working_quant_3")], by = "proposal_id")

## Look at disciplinary distance between reviewers
dpr <- dpr %>%
  rowwise() %>%
  mutate(matching_rev1_2 = case_when(rev_d1_1 == rev_d1_2 ~ 1, ## Within discipline
                              rev_d1_1 != rev_d1_2 ~ 0, ## Outside of discipline
                              TRUE ~ NA),
         matching_rev1_3 = case_when(rev_d1_1 == rev_d1_3 ~ 1, ## Within discipline
                              rev_d1_1 != rev_d1_3 ~ 0, ## Outside of discipline
                              TRUE ~ NA),
         matching_rev2_3 = case_when(rev_d1_2 == rev_d1_3 ~ 1, ## Within discipline
                              rev_d1_2 != rev_d1_3 ~ 0, ## Outside of discipline
                              TRUE ~ NA))
```

## Calibrating reviewer scores

Proposals were ranked by trimmed means. Other methods of combining
scores were also calculated including (raw) proposal mean and mean (mean
shift).

**vote:** Raw score for a proposal by a reviewer

**proposal_mean** : Mean vote given to a proposal

**reviewer_mean:** Mean score for each reviewer

**mean_of_mean:** Mean of all reviewer_means

Reviewer Adjustment (**r.adj**): mean_of_mean - reviewer_mean

**mean_shift**: reviewer adjustment + vote

E.g. I give scores of 5,7,4,4 so my reviewer_mean is 5. The
(hypothetical) average of average scores is 5.5 so my reviewer
adjustment is 0.5. My adjusted scores are therefore 5.5, 7.5, 4.5, 4.5

Means using raw vote (with some scores removed):

**trimmed_mean**= Trimmed mean vote of a proposal without 1 x highest
and 1x lowest score.

**proposal_mean_low** = Mean vote without lowest scoring reviewers

```{r}
## Calibrating scores using different methods  
## Define function to calculate adjusted scores 
calibrate_scores_function <- function(df, var) {
  df %>%
    group_by(proposal_id) %>%
    mutate(proposal_mean = mean(get(var), na.rm=T)) %>% ## Use proposal mean score as reference mean
    ungroup() %>%
    group_by(reviewer_id) %>%
    mutate(reviewer_mean = mean(get(var), na.rm =T)) %>%
    ungroup() %>%
    # Mean shift = mean of reviewers mean - reviewer mean + raw vote
    mutate(mean_of_mean = mean(reviewer_mean),
           r.adj = mean_of_mean - reviewer_mean, #Calculate difference between mean of all reviewers means and individual reviewer mean
           mean_shift = get(var) + r.adj) %>% ## Add adjustment to vote
    group_by(proposal_id) %>%
    mutate(mean_mean_shift = mean(mean_shift, na.rm = T),
           trimmed_mean = mean(sort(vote)[-c(1, length(vote))]), # Trim highest and lowest scores
           # drop votes from lowest scoring reviewers 
           proposal_mean_low = mean(vote[reviewer_mean>=3.5])) %>%  
    ungroup()
}

# Run function on dpr df
dpr_calibrated <- calibrate_scores_function(dpr, "vote") 

## Check distribution of ranking variables
dpr_calibrated %>% 
  pivot_longer(cols = c("vote", "mean_shift")) %>%
  group_by(name) %>%
  mutate(mean = mean(value)) %>%
  ggplot(aes(value)) + 
  geom_histogram(binwidth = 0.5)  + 
  facet_wrap(~name, scales = "free") 

## Plot distribution or reviewers' mean scores
dpr_calibrated %>%
  filter(!duplicated(reviewer_id)) %>% #only include each reviewer once
  ggplot(aes(reviewer_mean)) + 
  geom_histogram(binwidth =0.2) +
  geom_vline(aes(xintercept = mean(reviewer_mean)), color = "orange", linewidth = 1) +
  labs(subtitle = "Distribution of reviewers' mean scores", x = "Mean score per reviewer") 

## Check distribution of means by proposal (according to different adjusted scores)
dpr_calibrated %>% 
  filter(!duplicated(proposal_id)) %>% #only include each proposal once
  pivot_longer(cols = c("proposal_mean", "mean_mean_shift", "trimmed_mean", "proposal_mean_low")) %>%
  ggplot(aes(value)) + 
  geom_histogram(binwidth = 0.2, fill = "steelblue")  + 
  facet_wrap(~name, scales = "free") 


## Plot trimmed mean distribution with overall average
dpr_calibrated %>%
  filter(!duplicated(proposal_id)) %>%
ggplot(aes(trimmed_mean)) + 
  geom_histogram(binwidth =0.2, fill = "steelblue") +
  geom_vline(aes(xintercept = mean(trimmed_mean)), color = "darkred", linewidth = 1, linetype = "dashed") +
  scale_y_continuous(limits = c(0, 27), expand = c(0,0)) +
  scale_x_continuous(limits = c(1,9), breaks = c(1,2,3,4,5,6,7,8,9)) +
  labs(subtitle = "Distribution of proposals' trimmed means", x = "Trimmed mean score") 

```

####Ranking proposals 
Rank proposals according to trimmed mean, proposal mean and (mean) mean shift

```{r}
## Add sd and se for trimmed_mean to dpr_calibrated
dpr_calibrated %<>%
 group_by(proposal_id) %>%
  mutate(trimmed_sd = sd(sort(vote)[-c(1, length(vote))]),
         n_trimmed = n() - 2, # Because some have 10 votes and some have 9
         trimmed_se = trimmed_sd/ sqrt(n_trimmed)) %>% 
  ungroup()


## Define function to rank proposals
ranking_function <- function(df, var) {
  df %>%
    ## Rank proposals by means
    filter(!duplicated(proposal_id)) %>%  
    mutate(!!glue("rank_by_{var}") := round(rank(desc(get(var))), 0)) %>% # Rank in descending order (or lowest score = 1)
    ungroup()
}

## Run ranking function for trimmed mean (and by proposal mean and (mean) mean shift)
ranking_variables <- c("trimmed_mean","proposal_mean", "mean_mean_shift")

ranking_dfs <-  lapply(ranking_variables, function(ranking_variables) {
  ranking_function(dpr_calibrated, ranking_variables)
}) 

## Combine lists from lapply output
rankings_1 <- Reduce(function(x, y) merge(x, y, by = intersect(names(x),
                    names(y)), all = TRUE), ranking_dfs)

## Create extra trimmed mean ranking for plotting only. 
## Rank by trimmed mean, if ranks are tied then rank by standard error, if still tied by order of proposal
rankings_1b <-  rankings_1 %>%
  ungroup() %>%
  group_by(rank_by_trimmed_mean) %>%
  mutate(within_rank_SE = dense_rank(trimmed_se)) %>%
  ungroup() %>%
  arrange(rank_by_trimmed_mean, within_rank_SE) %>%
  mutate(plot_ID = row_number())

## Create table of rankings (and alternative rankings using proposal mean and mean shift) and plot_ID
rank_only <- rankings_1b %>%
  dplyr::select(c("proposal_id", starts_with("rank_by"), "plot_ID")) %>%
  arrange(rank_by_proposal_mean)

# Join rankings_1 to dpr_calibrated 
dpr_calibrated2 <- inner_join(dpr_calibrated, rank_only, by = "proposal_id")

## Create variables for whether a proposal was funded or not by different routes
dpr_calibrated2 <- dpr_calibrated2%>% 
  mutate(funded_DPR = case_when(rank_by_trimmed_mean <= 10 ~ 1,
                                        TRUE ~ 0),
                       funded_DPR = as.factor(funded_DPR))

## Filter prp to include shortlisted proposals only
prp_shortlisted <-prp %>% 
  mutate(funded_panel = case_when(funding == TRUE ~ 1,
                                              TRUE ~ 0),
                       funded_panel = as.factor(funded_panel))
prp_shortlisted2 <- prp_shortlisted %>%     
  distinct(proposal_id, .keep_all = T) # Get single list of proposal_id only

## Add panel stages information to main df
dpr_funded <- left_join(dpr_calibrated2, prp_shortlisted2[, c("proposal_id", "funded_panel", "shortlisted", "discussed", "funding", "qa_combined")], by = "proposal_id")

## Add variables for funded by any route and funded by both DPR and panel 
dpr_funded <- dpr_funded%>%
  mutate(funded_both = case_when(funded_panel ==1 & funded_DPR== 1 ~ 1, 
                                 TRUE ~ 0),
                       funded_both = as.factor(funded_both),
         funded_any = case_when (funded_DPR ==1 | funded_panel == 1 ~ 1,
                                 TRUE ~ 0))

## Add custom palette Color2 to dpr_funded for future plots
dpr_funded <- dpr_funded %>%
  mutate(Color2 = as.factor(case_when(
    funding == TRUE & rank_by_trimmed_mean <= 10 ~ "#5ebdd2", #DPR + panel funded
    funding == "TRUE" ~ "#ffa600", # Panel funded
    rank_by_trimmed_mean <= 10 ~ "#bc5090", # DPR funded
    TRUE ~ "lightgrey"))) # Not funded

```

## Overlap between DPR and Panel

```{r}
## Look at internal shortlisted and DPR top 70
comp <- dpr_funded %>%
  filter(!duplicated(proposal_id)) %>%
  dplyr::select(c("proposal_id", "rank_by_trimmed_mean", "trimmed_mean", "trimmed_se", "proposal_mean", 
           "rank_by_proposal_mean", "rank_by_mean_mean_shift", "shortlisted", "discussed", "funding", "qa_combined", "plot_ID"))%>%
  arrange(rank_by_trimmed_mean)

## Compare top 70 rankings and shortlisted proposals
comp %>%
  filter(rank_by_trimmed_mean < 71& shortlisted == "TRUE") %>% # Up to rank 63 (top 68 due to ties)
  count() # 41
41/68 * 100 # 60.29% overlap

## Compare discussed proposals with top 42
comp %>%
  filter(discussed == TRUE & rank_by_trimmed_mean <45) %>% # Top 43 DPR
  count() # 20
20/43*100 # 46.51%

```

***Panel - shortlisted and discussed proposals***

70 out of 140 proposals were shortlisted by VWS staff for the panel.
Prior to the panel all 70 shortlisted proposals were quick assessed by
panel members (2 per proposal).

-   45 out of 70 received at least one A

```{r}
## Visualise funded proposals by DPR rank
funded <- dpr_funded %>%
  filter(!duplicated(proposal_id)) %>%
  ggplot(aes(x = plot_ID, y = trimmed_mean, 
                         ymin = trimmed_mean - (qnorm(.975) * trimmed_se), 
                         ymax = trimmed_mean + (qnorm(.975) * trimmed_se), 
                         fill = Color2)) +
  geom_crossbar(width = 1, linewidth = 0.0001, color = "#5A5A5A") +# Use crossbar to show CI
  scale_fill_identity(labels = c("#5ebdd2" = "Panel & DPR funded\n(n=3)",
               "#ffa600" = "Panel funded\n(n=8)",
               "#bc5090" = "DPR funded\n(n=7)",
               "lightgrey" = "Not funded\n(n=122)"),
    guide = "legend", name = NULL) +
  labs(x = "\nRank", 
       y = "Trimmed mean with 95% CI\n", 
       title = "Trimmed mean (with 95% confidence intervals) of each proposal according to final rank\nunder DPR",
       caption = "Note: For the purpose of this visualization, where ranks were tied proposals were ordered by standard error.\nAny remaining ties were broken by proposal order") +
  scale_x_continuous(breaks = c(1, 20, 40, 60, 80, 100, 120, 140)) +
  scale_y_continuous(breaks = seq(2, 9, 1)) +
  theme(plot.caption = element_text(hjust = 0))
  
funded
```

```{r}
## Visualise from the panel perspective
## Create new df with column to indicate stage reached (not shortlisted, shortlisted, discussed or funded)
joined_panel <- dpr_funded %>%
  filter(!duplicated(proposal_id)) %>%
  mutate(status = case_when(shortlisted == FALSE ~ 1,
                            shortlisted == TRUE & discussed == FALSE ~ 2,
                            discussed == TRUE & funding == FALSE ~ 3,
                            funding == TRUE ~ 4),
         status_fact = factor(status, levels = c(1, 2, 3, 4)),
         status_fact = fct_recode(status_fact, "Not shortlisted" = "1",
                                   "Shortlisted" = "2",
                                   "Discussed" = "3",
                                   "Funded" = "4"))
## Add info for plotting
joined_panel2 <- joined_panel %>%
  arrange(status_fact, -rank_by_trimmed_mean) %>%
  mutate(panel_plt_no = row_number(), # To avoid overlaps
         x_offset = case_when( # Add offsets to give gaps between groups
    status_fact == "Funded" ~ 9,  
    status_fact == "Discussed" ~ 6,  
    status_fact == "Shortlisted" ~ 3,  
    status_fact == "Not shortlisted" ~ 0,  
  )) 
 
## Plot panel progress
ggplot(joined_panel2, aes(x=panel_plt_no, y=status)) +
        geom_segment(data = filter(joined_panel2, status_fact == "Funded"), aes(x = panel_plt_no + x_offset, y = 0 , yend = status, color = Color2), linewidth = 1.2) +
   geom_segment(data= filter(joined_panel2, status_fact == "Discussed"), aes(x = panel_plt_no + x_offset, y = 0, yend = status, color = Color2), linewidth = 1.2) +
     geom_segment(data= filter(joined_panel2, status_fact == "Shortlisted"), aes(x = panel_plt_no + x_offset, y = 0, yend = status, color = Color2), linewidth = 1.2)   +
       geom_segment(data= filter(joined_panel2, status_fact == "Not shortlisted"), aes(x = panel_plt_no + x_offset, y = 0, yend = status, color = Color2), linewidth = 1.2) +
  scale_color_identity(labels = c("#5ebdd2" = "Panel & DPR funded\n(n=3)",
               "#ffa600" = "Panel funded\n(n=8)",
               "#bc5090" = "DPR funded\n(n=7)",
               "lightgrey" = "Not funded\n(n=122)"), guide = "legend", name = NULL) + 
   scale_x_reverse(breaks= c(145, 120, 87, 40),
                   labels = c("Funded\n(n=11)\n", "Discussed\n(n=31)\n", "Shortlisted\n(n=28)\n", "Not shortlisted\n(n=70)\n")) +
   scale_y_continuous(breaks = c(1,2,3,4),
                      labels = c("Not shortlisted",
                                 "Shortlisted",
                                 "Discussed",
                                 "Funded")) +
   labs(title = "Proposal progression under panel according to furthest stage reached\n(shortlisted, discussed or funded)", y = NULL, x = "Stage of panel process")
```

## Stability of DPR
Use bootstrapping (with resampling) to assess stability of ranking/funding decision.
```{r}
## Bootstrap 500 samples of 'vote' 
## Select variables 
dpr2 <- dpr_funded %>%
  dplyr::select("proposal_id", vote)

## Group by proposal_id and get 500 samples (with resampling)
set.seed(123) 
alt_sample <- replicate(500, {
  dpr2 %>%
    group_by(proposal_id) %>%
    sample_n(size = n(), replace = TRUE) %>% 
    ungroup() 
}, simplify = FALSE)

# Join samples together  
alt_df<- bind_rows(alt_sample, .id = "sample_id")  

## Define function to calculate trimmed_means
trimming_funct <- function(x) {
 mean(sort(x)[-c(1, length(x))])
} 

# Get trimmed mean by proposal and by sample
trimmed_alt <- alt_df %>%
  group_by(sample_id,proposal_id) %>%
  summarize(trimmed_meanBoot = trimming_funct(vote), .groups = 'drop')

## For each sample rank proposals
trimmed_ranked <- trimmed_alt %>% 
  group_by(sample_id) %>%
  filter(!duplicated(proposal_id)) %>%
  mutate(ranking = round(rank(desc(trimmed_meanBoot)), 0)) %>%
  ungroup()

## Join with original ranks and trimmed mean and funded status - use joined_panel df to avoid duplicate proposal_ids and many-to-many relationships
boot_rankings <- left_join(trimmed_ranked, joined_panel[,c("proposal_id", "trimmed_mean", "rank_by_trimmed_mean", "funded_DPR", "funded_panel", "funded_both", "funded_any")], by = "proposal_id")

```

**Understanding percentage of samples which continue to be funded**

```{r}
## Define function to calculate "survival" percentage
survival_function <- function(df, var) {
  df %>%
    group_by(sample_id) %>%
    mutate(count_greater = sapply(get(var), function(x) sum(get(var)>x))) %>% # count number of proposals with greater trimmed mean than target 
    ungroup() %>%
    group_by(proposal_id) %>%
    # Proposals which are funded
    mutate(survival_count = sum(ranking <= 10 | count_greater < 10),# count_greater <10 accounts for ties
           survival_perc = (survival_count/500)*100) %>%
    ungroup() %>%
    # Get no. and percentage of proposals funded under DPR which are still funded for each sample
    group_by(sample_id) %>%
    mutate(survival_per_sample = sum((ranking <= 10 | count_greater < 10)& rank_by_trimmed_mean <=10),
           survival_perc_sample = (survival_per_sample/500)*100) %>%
    ungroup()
}

# Run function on boot_Rankings
funded_survival <- survival_function(boot_rankings, "trimmed_meanBoot")

## Get mean, sd and se of DPR survival per sample
print(avg_survived <- mean(funded_survival$survival_per_sample))
print(sd_survived <- sd(funded_survival$survival_per_sample))
se_survived <- sd_survived/sqrt(500) 

## Mark whether proposals are always, sometimes or never funded (across all samples)
funded_survival <- funded_survival %>% 
  mutate(funded_surv = case_when(survival_perc >=100  ~ "Always",
           survival_perc >0 & survival_perc <100 ~ "Sometimes",
           survival_perc == 0 ~ "Never",
           TRUE ~ NA)) 

  ## Get frequencies of always/never/sometimes
survival_freqs <- funded_survival %>% 
  group_by(funded_surv) %>%
  summarise(count = n()) %>%
  mutate(percentage = round((count/sum(count)*100), 2)) %>%
  ungroup()

# Get frequencies per survival_per_sample
survival_frequencies <- funded_survival %>%
  distinct(sample_id, survival_per_sample) %>%
  group_by(survival_per_sample) %>%
    summarise(count = n(), .groups = "drop") %>%
  arrange(desc(survival_per_sample)) %>%
    mutate(percentage = count / sum(count)*100,
           cumulative_count = cumsum(count),
           cumulative_percent = cumsum(count)/sum(count)*100 ) %>%
    ungroup()

```

### Changing number of reviewers

Repeated bootstrapping procedure but varied the number of reviewers (3
to 20) obtaining 500 samples for each number of reviewers.

```{r}
# Define function to obtain bootstrap sample and change no reviewers
boot_sample_raters <- function(data, n_raters) {
  data %>%
    group_by(proposal_id) %>%
    slice_sample(n = n_raters, replace = TRUE) %>%
    ungroup()
}

# Get 500 samples using boot_sample_raters
set.seed(123)
bootstrap_samples <- map(3:20, function(n_raters) {
  map(1:500, function(sample_number){
    sample_df <- boot_sample_raters(dpr2, n_raters)
    sample_df %>%
      mutate(rater_id = n_raters, sample_id = sample_number)
    })
})

# Join samples together  
raters_boot<- bind_rows(flatten(bootstrap_samples))  

# Get mean and trimmed mean by proposal, no. reviewers, and  sample
raters_boot2<- raters_boot %>%
  group_by(sample_id, rater_id, proposal_id) %>%
  mutate(trimmed_mean2 = trimming_funct(vote),
         mean2 = mean(vote))

## Rank by mean and trimmed mean (by sample and no reviewers)
raters_ranked <- raters_boot2 %>% 
  group_by(sample_id, rater_id) %>%
   filter(!duplicated(proposal_id)) %>%
  mutate(ranking = round(rank(desc(trimmed_mean2)), 0),
         ranking_mean = round(rank(desc(mean2)), 0)) %>%
  ungroup()

# Join to original rankings by trimmed mean and proposal mean
raters_ranked2 <- left_join(raters_ranked, rankings_1[,c("proposal_id", "trimmed_mean", "rank_by_trimmed_mean", "proposal_mean", "rank_by_proposal_mean")], by = "proposal_id")

## Define survial function2 (Accounting for multiple raters)
survival_function2 <- function(df, var) {
  df %>%
    group_by(sample_id, rater_id) %>%
    mutate(count_greater = sapply(get(var), function(x) sum(get(var)>x))) %>%
    ungroup() %>%
    group_by(proposal_id, rater_id) %>%
    # Proposals which are funded
    mutate(survival_count = sum(ranking <= 10 | count_greater < 10),
           survival_perc = (survival_count/500)*100) %>%
    ungroup() %>%
    # Count no proposals which are still funded for each sample
    group_by(sample_id, rater_id) %>%
    mutate(survival_per_sample = sum((ranking <= 10 | count_greater < 10)& rank_by_trimmed_mean <=10),
           survival_perc_sample = (survival_count/500)*100) %>%
    ungroup() %>%
    # Get average no previously funded proposals which are still funded by no.reviewers
    group_by(rater_id) %>%
    mutate(survival_per_rater = mean(survival_per_sample),
           survival_rater_se = sd(survival_per_sample)/sqrt(500)) %>%
    ungroup()
}

# Run survival function on raters_ranked2
survival_boot <- survival_function2(raters_ranked2, "trimmed_mean2")

# Mark whether proposals which were originally funded/not funded are always, sometimes,never funded in samples
survival_boot2<- survival_boot %>%
  group_by(proposal_id, rater_id) %>%
  mutate(orig_funded = case_when(
           survival_perc >=100 & rank_by_trimmed_mean <=10 ~ "Always",
           survival_perc >0 & survival_perc <100 & rank_by_trimmed_mean <=10 ~ "Sometimes",
           survival_perc == 0 & rank_by_trimmed_mean <=10 ~ "Never",
           TRUE ~ NA),
         not_funded = case_when(
           survival_perc >=100 & rank_by_trimmed_mean >10 ~ "Always",
           survival_perc >0 & survival_perc <100 & rank_by_trimmed_mean >10 ~ "Sometimes",
            survival_perc == 0 & rank_by_trimmed_mean >10 ~ "Never",
           TRUE ~ NA)) %>%
  ungroup()

```

Visualising the average number of originally funded proposals funded
according to different numbers of reviews per proposal

```{r}
## Plot the mean no. originally funded proposals which "survived" according to 
mean_survival <- survival_boot2 %>%
    filter(rater_id>2 & !duplicated(rater_id)) %>%  
  arrange(-rater_id) %>%
ggplot(aes(y=survival_per_rater, x= rater_id)) +
  geom_point(color = "#0d7d87", size = 0.8) + 
  geom_errorbar(aes(ymin =  survival_per_rater -(qnorm(.975) * survival_rater_se), 
                    ymax = survival_per_rater +(qnorm(.975)*survival_rater_se)), 
                color = "#0d7d87", linewidth = 0.8, width = 0.3)+
  scale_x_continuous(breaks = seq(3,20, 1)) +
   ylim(3,7.5) +
  labs(x = "Number of reviews\n", y = "Mean no. of proposals still funded (with 95% CIs)\n", 
       title = "Mean number of originally DPR funded proposals still funded across 500 samples\naccording to number of reviews per proposal",
       caption = "*500 samples obtained for each possible number of reviews") +
  theme(caption = element_text(hjust = 0), plot.title = element_text(size = 12))
#ggsave("surviva_plt1.png")
mean_survival
```

###Quartile Agreement Matrices - Based on+ DPR
<https://www.eso.org/sci/publications/messenger/archive/no.177-sep19/messenger-no177-3-13.pdf>

QAM for 1 X DPR and 1X panel reviewer for 70 shortlisted proposals only

```{r}
## Repeat QAM using 1X DPR reviewer and 1X panel reviewer
## Select dpr proposals which were shortlisted (so got quick assessments)
funded_QAM <- dpr_funded %>%
  filter(shortlisted == TRUE) %>%
  dplyr::select(proposal_id, vote)

## Group by proposal_id and get 500 samples of n=1 for vote (dpr) 
set.seed(123) # set seed for reproducibility
QAM_sample <- replicate(500, {
  funded_QAM %>%
    group_by(proposal_id) %>%
    sample_n(size = 1, replace = FALSE) %>% 
    ungroup() 
}, simplify = FALSE)

# Join QAM samples together  
QAM_dpr<- bind_rows(QAM_sample, .id = "sample_id")  

## Select vote.y and proposal_id from prp shortlisted2
prp_QAM <- prp %>%
  filter(shortlisted == TRUE) %>%
  dplyr::select(proposal_id, vote.y)

## Group by proposal_id and get 500 samples of n=1 for vote.y
set.seed(123) # set seed for reproducibility
QAM_sample2 <- replicate(500, {
  prp_QAM %>%
    group_by(proposal_id) %>%
    sample_n(size = 1, replace = FALSE) %>% 
    ungroup() 
}, simplify = FALSE)

# Join QAM samples together  
QAM_panel<- bind_rows(QAM_sample2, .id = "sample_id")  

## Join QAM_dpr and QAM_panel
QAM_df <- left_join(QAM_dpr, QAM_panel, by = c("proposal_id", "sample_id"))

## Rank proposals by sample
QAM_ranked <- QAM_df %>% 
  group_by(sample_id) %>%
  mutate(ranking_dpr = round(rank(desc(vote)), 0),
         ranking_qa = round(rank(desc(vote.y)),0)) %>%
  ## Assign numbers to artificially break ties
  arrange(desc(ranking_dpr)) %>%
  mutate(rankdpr_no = row_number()) %>%
  arrange(desc(ranking_qa)) %>%
  mutate(rankqa_no = row_number()) %>%
  ## Categorise each set of rankings (per sample) by quartile - do this by number
  ##to force equal size quartile (otherwise they won't be, due to ties)
  mutate(quartile_rankingdpr = case_when(rankdpr_no <=18 ~ 1,
                                       rankdpr_no >18 & rankdpr_no <=35 ~2,
                                       rankdpr_no >35 & rankdpr_no <=53 ~3,
                                       rankdpr_no >53 & rankdpr_no <=70 ~ 4),
         quartile_rankingqa = case_when(rankqa_no <=18 ~ 1,
                                       rankqa_no >18 & rankqa_no <=35 ~2,
                                       rankqa_no >35 & rankqa_no <=53 ~3,
                                       rankqa_no >53 & rankqa_no <=70 ~ 4)) %>%
    ungroup()

## Calculate quartile agreement per sample and then get an average
agreement <- QAM_ranked %>%
    group_by(sample_id) %>%
    summarise(counts = list(table(quartile_rankingdpr, quartile_rankingqa))) %>%  # Get frequencies of all possible combinations
    ungroup() %>% 
  mutate(counts = lapply(counts, as.data.frame)) %>% #make a dataframe, unnest won't work with a contingency table
    unnest(cols = c(counts)) %>%# Unnest the proportions 
  group_by(sample_id) %>%  #Group and calculate proportions
  mutate(prop = Freq/ ifelse(quartile_rankingdpr == 1 | quartile_rankingdpr ==3, 18, 17)) %>% # 17 or 18 proposals per quartile
  ungroup()%>%
  group_by(quartile_rankingdpr, quartile_rankingqa) %>%  
  summarise(average_proportion = round(mean(prop), 2))  # Calculate the average proportion for each combination

## Define function to turn proportions into a matrix
matrix_funct <- function(df, var1, var2, var3) {
  # Pivot the data frame into a matrix
  matrix_result <- df %>%
    pivot_wider(names_from = var2, values_from = var3)

  # Convert the result into a matrix
  matrix_result <- as.matrix(matrix_result[, -1])  # Remove the var1 column and   convert to matrix
  
  print(matrix_result)
}

## Run matrix_funct on agreement2
agreement_matrix <- matrix_funct(agreement, "quartile_rankingdpr", "quartile_rankingqa", "average_proportion")

```

## Characterising reviewing

```{r}
## Define frequency function
freq_function<- function(df, var) {
  df %>%
    group_by(get(var)) %>%
    summarise(count = n()) %>%
    mutate(percentage = round((count/sum(count) * 100), 2))
}

## Check distribution of per proposal, per reviewer numeric variables (DPR)
dpr_funded %>% 
  pivot_longer(cols = c(crit1:others_avg, uncertainty, numeric_dFit, overlap )) %>%
  group_by(name) %>%
  mutate(mean = mean(value)) %>%
  ggplot(aes(value)) + 
  geom_histogram(binwidth = 0.6) + 
  facet_wrap(~name, scales = "free") 

## Save numeric variables as vectors
dpr_vars <- c("crit1", "crit2", "crit3", "crit4", "vote", "conf_lowest",
              "conf_highest", "others_avg","review_time",  "numeric_dFit", "overlap")
panel_vars <- c( "crit1", "crit2", "crit3", "crit4", "vote.y", "conf_lowest",
                 "conf_highest", "others_avg","review_time", "uncertainty", "numeric_dFit")

#Function for SE
se <- function(x){
  sd(x) / sqrt(length(x))
}

## Get averages for dpr and panel using st from vtable
dpr_descriptives <- st(dpr_funded, summ = c("mean(x)", "sd(x)", "se(x)",
                                            "median(x)", "IQR(x)"),
    vars = dpr_vars, digits = 3, out = "return")

panel_descriptives <- st(prp, summ = c("mean(x)", "sd(x)", "se(x)", "median(x)",
                                       "IQR(x)"),
    vars = panel_vars, digits = 3, out = "return")

## Get correlations for all per reviewer per proposal numeric variables for DPR
dpr_corr <- dpr_funded %>%
  dplyr::select(crit1:others_avg, numeric_dFit, review_time, overlap) %>%
  tab_corr(triangle = "lower", title = "Correlations between numeric variables for DPR")


## Get correlations between numeric variables for panel
panel_corr <- prp %>%
  filter(shortlisted == TRUE) %>% # Only include proposals shortlisted
  dplyr::select(qa_rev1, qa_rev2, vote.x, crit1:others_avg, review_time) %>%
  tab_corr(triangle = "lower", title = "Correlations between numeric variables for panel")


##Define function for basic scatterplot
scatter_funct<- function(df, x_var, y_var) {
  ggplot(df, aes(x = {{x_var}}, y = {{y_var}})) + 
    geom_point() +  # Add scatter plot points
    geom_smooth(method = "lm", se = FALSE, color = "blue") + 
    labs(x = deparse(substitute(x_var)), y = deparse(substitute(y_var)), 
         title = paste("Scatterplot of", deparse(substitute(x_var)), "against",
                       deparse(substitute(y_var))))
}
```

### Efficiency of DPR/Panel

Time spent reviewing proposals under DPR

```{r}
## Total time DPR
DPR_time <- (sum(dpr_funded$review_time)/ 60)
DPR_one <- median(dpr_funded$review_time)/60
DPR_oneIQR <- IQR(dpr_funded$review_time)/60

## Total time panel
shortlisting_time <- 0.75*140
qa_time <- (sum(panel_qa$review_time)/ 60)
panel_time <- shortlisting_time + qa_time

## Get total + average time per proposal for DRP
dpr_funded %<>%
  group_by(proposal_id) %>%
  mutate(total_time_proposal = sum(review_time),
           average_time_proposal = median(review_time)) %>%
  ungroup() %>%
## Total and average time per reviewer
  group_by(reviewer_id) %>%
  mutate(total_time_reviewer = sum(review_time),
         average_time_reviewer = median(review_time)) %>%
  ungroup()

## Get median and IQR total time per reviewer
median_reviewer_hours = median(dpr_funded$total_time_reviewer)/60
IQR_hours = IQR(dpr_funded$total_time_reviewer)/60
max_reviewer_hours = max(dpr_funded$total_time_reviewer)/60

## Median time per proposal
median_proposal_hours = (median(dpr_funded$total_time_proposal, na.rm = T)/60)

## Get total + mean time per reviewer/proposal for panel
prp_short <-prp %>%
  filter(shortlisted == TRUE) %>%
  ## Get total + average time per reviewer for panel
  group_by(reviewer_id) %>%
  mutate(total_time_reviewer = sum(review_time),
         average_time_reviewer = mean(review_time)) %>% 
  ungroup() %>%
  ## Get total + average time per proposal for panel
  group_by(proposal_id)%>%
  mutate(total_time_proposal = sum(review_time),
           average_time_proposal = mean(review_time)) %>%
    ungroup()

## Median total time per panel proposal and reviewer
median_panel_proposal = median(prp_short$total_time_proposal, na.rm = T)/60
median_panel_reviewer_hours = median(prp_short$total_time_reviewer, na.rm = T)/60
mean_panel_reviewer = mean(prp_short$total_time_reviewer, na.rm = T)/60
max_panel_reviewer_hours = max(prp_short$total_time_reviewer)/60

```

### Reviewer characteristics

```{r}
## Frequencies and summary statistics
## Create dpr_calibrated df including each reviewer just once
dpr_reviewer <- dpr_funded %>%
  filter(!duplicated(reviewer_id)) 

dpr_reviewer <- dpr_reviewer %>%
  mutate(gender= if_else(is.na(gender), "No answer", gender))
  
## Get frequency of gender, first-gen and ECR status
rev_gen_freq <- freq_function(dpr_reviewer, "gender")

rev_first_freq <- freq_function(dpr_reviewer, "first_gen")

rev_ecr_freq <- freq_function(dpr_reviewer, "ecr")

rev_years_freq <- freq_function(dpr_reviewer, "working_quant")


# Get stats for working years
dpr_reviewer %>%
  ungroup() %>%
  summarize(mean_working = mean(working_yrs, na.rm = T),
            sd_working = sd(working_yrs, na.rm = T),
            med_working = median(working_yrs, na.rm = T),
            IQR_working = IQR(working_yrs, na.rm = T))           

freq_function(dpr_reviewer, "working_yrs")

## Distribution of working years
dpr_reviewer %>%
  filter(!is.na(working_yrs)) %>%
ggplot(aes(working_yrs)) + 
  geom_histogram(binwidth = 2, fill = "steelblue") +
  scale_y_continuous(expand = c(0,0), limits = c(0.0, 50)) +
    scale_x_continuous(breaks= seq(0, 55, 5)) +
  labs(subtitle = "Distribution of years spent working in research among DPR applicants/reviewers (N =322*)", 
       x = "Working years", Y = "Count",
       caption ="*No response for one reviewer") 

```

***Statistics according to reviewer level variables***

```{r}
## Summary stats for reviewer level variables
st(dpr_funded, summ = c("mean(x)", "sd(x)", "median(x)", "IQR(x)"), 
   vars = c("reviewer_range", "avg_uncertainty", "total_time_reviewer","average_time_reviewer"),
  digits = 3, out = "return")

## Get summary statistics according to gender
st(dpr_funded, summ = c("mean(x)", "sd(x)", "median(x)", "IQR(x)"), group = "gender",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time",
            "proposal_range", "numeric_dFit", "working_yrs"),
  digits = 3, out = "return", title = "Statistics by gender")

# Get summary statistics according to first gen status
st(dpr_funded, summ = c("mean(x)", "sd(x)", "median(x)", "IQR(x)"), group = "first_gen",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time",
            "proposal_range", "numeric_dFit", "working_yrs"),
  digits = 3, out = "return", title = "Statistics by first generation status")

## Get summary statistics according to working years
st(dpr_funded, summ = c("mean(x)", "sd(x)"), group = "working_quant",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time","numeric_dFit",
            "reviewer_range", "working_yrs", "r.adj"),
  digits = 3, out = "return", title = "Statistics by reviewer (first) discipline")

## Get summary statistics according to within/outside of discipline
st(dpr_funded, summ = c("mean(x)", "sd(x)"), group = "matching",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", "conf_highest",
            "uncertainty", "others_avg", "review_time","numeric_dFit", "reviewer_range",
            "working_yrs", "r.adj"),
  digits = 3, out = "return", title = "Statistics by within/outside discipline")

## Get summary statistics according to self_rated appropriateness to review
st(dpr_funded, summ = c("mean(x)", "sd(x)"), group = "disciplinary_fit",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time", 
            "reviewer_range", "working_yrs", "r.adj"),
  digits = 3, out = "return")
```

```{r}
## Get proposal only version of dpr_funded
dpr_proposal <- dpr_funded %>%
  ungroup() %>%
  filter(!duplicated(proposal_id))

## Data frame of dpr funded only
dpr_funded_DPR <- dpr_proposal %>%
  filter(funded_DPR == 1)

## Data frame of panel funded only
dpr_funded_panel <- dpr_proposal %>%
  filter(funded_panel == 1)

## Get all disciplines of DPR funded proposals
dpr_alldis <- as.data.frame(table(unlist(dpr_funded_DPR$pro_dds)))
  
## Get first discipline of DPR funded proposals
table(dpr_funded_DPR$pro_d1)

## Get all disciplines of panel funded proposals
panel_alldis <- as.data.frame(table(unlist(dpr_funded_panel$pro_dds)))
  
## Get first discipline of funded proposals
table(dpr_funded_panel$pro_d1)
```

#### Disciplinary Overlap

matching = whether a reviewer was classed as within or outside of
discipline

overlap_scale = 0 to 4(theoretically 6) according to number of shared
disciplines between a reviewer and proposal

```{r}
## Specify levels of disciplinary fit 
dpr_funded <- dpr_funded %>% 
  mutate(disciplinary_fit = factor(disciplinary_fit,
                                   levels = c("Certain that I was inappropriate to provide a review", 
                                              "Not confident", "Low confidence",
                                              "Somewhat confident", "Confident",
                                              "Very confident", "Totally confident")),
         disciplinary_fit = fct_recode(disciplinary_fit, "Certain inappropriate" =
         "Certain that I was inappropriate to provide a review"))

## Get summary statistics according to whether within/outside of discipline
st(dpr_funded, summ = c("mean(x)", "sd(x)", "median(x)", "IQR(x)"), group = "matching",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time", "reviewer_range",
            "proposal_range", "avg_uncertainty", "numericdFit"),
  digits = 3, out = "return")

## Plot vote by matching
dpr_funded %>%
  mutate(matching = factor(matching, levels = c(1, 0))) %>%
  ggplot(aes(matching, vote)) +
  geom_boxplot(fill = "steelblue" )+
  geom_jitter(color="black", size=0.4, alpha=0.9) +
  scale_x_discrete(labels = c("Within discipline\nn=940", "Outside of discipline\nn=447")) +
    labs(x = NULL, y="Score", 
         title = "Scores according to whether reviewer was 
         classified as within or outside of\nproposal discipline") +
  scale_y_continuous(breaks = c(1, 2,3 ,4 ,5 ,6 ,7 ,8 ,9), limits = c(1,9)) +
     theme(plot.title = element_text(size = 14), 
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 12))

## Run regression to look at association between matching and vote
dpr_funded$matching <- as.factor(dpr_funded$matching)
summary(match_mod <- lm(vote ~ matching, data = dpr_funded))
plot(match_mod)
# Look at outliers
influenceIndexPlot(match_mod)
outlierTest(match_mod)

## Repeat with robust SEs
summary(match_modRob <- lm_robust(vote ~ matching, data = dpr_funded))

## Get average number of within and outside discipline reviewers proposal
dpr_funded %>%
  group_by(proposal_id, matching) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  group_by(matching) %>%
  summarise(mean = mean(count),
            sd = sd(count))

## Define function to format regression tables 
my_table <- function(mod, names) {
  tbl_regression(mod, include = everything(),
                 pvalue_fun = label_style_pvalue(digits = 3),
                 estimate_fun = label_style_number(digits = 2), label = names) 
}

```

```{r}
## Get freq of disciplinary fit
## For DPR
dpr_discp <- freq_function(dpr_funded, "disciplinary_fit")
tab_df(dpr_discp, title = "DPR Disciplinary Fit")

## For panel
## Filter prp_shortlisted to include only shortlisted proposals
prp_shortlisted3 <- prp_shortlisted %>%
  filter(shortlisted == TRUE)

prp_discp <- freq_function(prp_shortlisted3, "disciplinary_fit")

```

## Between reviewer agreement/consistency

```{r}
#Pivot data wider 
dpr_wide <- dpr_funded %>%
  dplyr::select(proposal_id, vote, reviewer_id) %>%
  group_by(proposal_id) %>%
  pivot_wider(values_from = vote,
              names_from = reviewer_id,
              values_fn = list(vote = ~ first(.)))

## Fit a model with vote predicted by random intercepts for proposals 
## (variation between proposals) and reviewers 
agree_mod <- lmer(vote ~ 1+  (1|proposal_id), dpr_funded) 
summary(agree_mod)
icc(agree_mod)

## Refit model adding in random effects of reviewer 
agree_mod2 <- lmer(vote ~ 1+  (1|proposal_id) + (1|reviewer_id), dpr_funded) # Can't nest reviewers within proposals
agree_mod2
summary(agree_mod2) 
confint(agree_mod2)

## Extract variance estimates and calculate ICC 
VarCorr(agree_mod2) %>%
  as.data.frame() %>%
  mutate(icc = vcov/sum(vcov))  # Calculate variance partitioning coefficient

## Use performance to calculate ICC
icc(agree_mod2, by_group = TRUE)

## Plot random effects
reEX <- REsim(agree_mod2) # Random effects estimates
plotREsim(reEX)

## Extract residuals
agree_resid <- residuals(agree_mod2)
head(agree_resid)

#Plot residuals - histogram and qqplot to check normality
hist(agree_resid)
qqnorm(agree_resid)
qqline(agree_resid)

## Plot residuals against fitted
plot(fitted(agree_mod2), residuals(agree_mod2))

## Plot random effects
ranef_model <- ranef(agree_mod2, condVar = TRUE)
qqnorm(ranef_model$proposal_id[[1]], main = "QQ-plot of proposal random effect")
qqnorm(ranef_model$reviewer_id[[1]], main = "QQ-plot of reviewer random effect")

## Compare agree_mod(unconditional) and agree_mod2
anova(agree_mod2, agree_mod)

```

```{r}
## Refit mod2 adding in 'matching' status as interaction with reviewer
agree_mod3 <- lmer(vote ~ 1+  (1|proposal_id) + (1|reviewer_id:matching) , dpr_funded) 
summary(agree_mod3) 

## Extract variance estimates and calculate ICC 
VarCorr(agree_mod3) %>%
  as.data.frame() %>%
  mutate(icc = vcov/sum(vcov))
#reviewer_id:matching = 19.47% variance
#proposal_id = 8.98%

## Plot random effects
reEX <- REsim(agree_mod3) # Random effects estimates
plotREsim(reEX)

## Compare agree_mod and agree_mod2
anova(agree_mod3, agree_mod)

```

Rerun mixed effects models excluding trimmed reviews

```{r}
## Get df without votes that would be trimmed
trimmed_dpr <- dpr_funded %>%
  group_by(proposal_id) %>%
  arrange(vote) %>%
  slice(-c(1, n())) %>%
  ungroup()

## Refit agree_mod and agree_mod2
agree_mod4 <- lmer(vote ~ 1+  (1|proposal_id), trimmed_dpr) 
summary(agree_mod4)
icc(agree_mod4)

## Refit model adding in random effects of reviewer 
agree_mod5 <- lmer(vote ~ 1+  (1|proposal_id) + (1|reviewer_id), trimmed_dpr) 
summary(agree_mod5) 
confint(agree_mod5)

## Extract variance estimates and calculate ICC 
VarCorr(agree_mod5) %>%
  as.data.frame() %>%
  mutate(icc = vcov/sum(vcov))  # Calculate ICC

## Use performance to calculate ICC
icc(agree_mod5, by_group = TRUE)

## Plot random effects
reEX2 <- REsim(agree_mod5) # Random effects estimates
plotREsim(reEX2)

## Extract residuals
agree_resid2 <- residuals(agree_mod5)
head(agree_resid2)

#Plot residuals - histogram and qqplot to check normality
hist(agree_resid2)
qqnorm(agree_resid2)
qqline(agree_resid2)

## Plot residuals against fitted
plot(fitted(agree_mod5), residuals(agree_mod5))

## Plot random effects
ranef_model2 <- ranef(agree_mod5, condVar = TRUE)
qqnorm(ranef_model2$proposal_id[[1]], main = "QQ-plot of proposal random effect")
qqnorm(ranef_model2$reviewer_id[[1]], main = "QQ-plot of reviewer random effect")

## Add in 'matching' status as interaction with reviewer
agree_mod6 <- lmer(vote ~ 1+  (1|proposal_id) + (1|reviewer_id:matching), trimmed_dpr) 
summary(agree_mod6)
## Compare agree_mod(unconditional) and agree_mod2
anova(agree_mod6, agree_mod4)

```

## Use of funding criteria

```{r}
## Do scores on each criteria predict overall vote?
summary(crit_mod_dpr <- lm(vote ~ crit1 + crit2 + crit3 + crit4, data = dpr_funded))
vif(crit_mod_dpr)
plot(crit_mod_dpr, which = c(1,2, 4, 5))

## Regression with robust SEs
summary(crit_mod_dpr_rob <- lm_robust(vote ~ crit1 + crit2 + crit3 + crit4, data = dpr_funded))

## Remind me what correlations between criteria were?
dpr_funded %>%
  ungroup() %>%
  dplyr::select(crit1, crit2, crit3, crit4) %>%
  tab_corr(triangle = "lower", title = "Correlations between scoring criteria")

# Visualise regression
avPlots(crit_mod_dpr) # Added variable plots
plot_coefs(crit_mod_dpr) # Plot coefficients

# Look at residuals
residualPlots(crit_mod_dpr)
marginalModelPlots(crit_mod_dpr)

# Look at outliers
influenceIndexPlot(crit_mod_dpr)
outlierTest(crit_mod_dpr)

## Try repeating regression without outliers 558, 614 and 1000
subset <- dpr_funded[-c(558, 614,1000, 35, 964), ]
summary(crit_mod_dpr2 <- update(crit_mod_dpr, data =  subset))
plot(crit_mod_dpr2, which = c(1,2,5)) # Not improved

## Try repeating regression with transformed DV
summary(crit_mod_dpr3 <- lm(vote^2 ~ crit1^2 + crit2^2 + crit3^2 + crit4^2, data = dpr_funded)) # Not improved
plot(crit_mod_dpr3) 
```

```{r}
## Repeat regression for panel
summary(crit_mod_prp <- lm(vote.y ~crit1 + crit2 + crit3 + crit4, data = prp_shortlisted3))
vif(crit_mod_prp)
plot(crit_mod_prp)
tab_model(crit_mod_prp, show.se = T, title = "Association between criteria scores and overall score")
## Regression with robust SEs
summary(crit_mod_prp_rob <- lm_robust(vote.y ~crit1 + crit2 + crit3 + crit4,
                                      data = prp_shortlisted3))
## Plot coefficients 
criteria_plt2 <-plot_summs(crit_mod_dpr_rob, crit_mod_prp_rob,
           model.names = c("DRP", "Panel"),
            coefs = c("Exploratory nature and novelty" = "crit1", 
                      "Added value of the constellation" = "crit2",
                      "Originality" = "crit3", 
                      "Scientific quality" = "crit4"),
           color= "Qual1")
criteria_plt2 + labs(title = "Criteria scores as predictors of overall score", 
                     caption = expression("DPR"*" R"^2*"= 0.929  "*"Panel"*" R"^2*"= 0.942",
                                          guide = NULL)) +
    theme(axis.text.y =  element_text(size = 12),
        axis.title = element_text(size = 14),
        plot.caption = element_text(size = 12),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.text = element_text(size = 12))

```

## Reviewer Uncertainty

```{r}
## Plot and get frequencies for reviewer uncertainty (uncertainty) and average uncertainty per reviewer for DPR
## Plot distribution of uncertainty
dpr_funded %>%  
  ggplot(aes(uncertainty)) + 
  geom_histogram(bins = 8, fill = "steelblue")+
  geom_vline(aes(xintercept = mean(uncertainty)), color = "darkred", 
             linetype = "dashed", linewidth = 1) +
  scale_y_continuous(expand =c(0,0), limits = c(0, 540)) +
  scale_x_continuous(breaks = c(0, 1,2,3,4,5,6,7, 8),
                     labels = c("0\nNo uncertainty", "1", "2", "3", "4", "5", "6",
                                "7", "8\nMaximum\nuncertainty")) +
  annotate("text", x= 4.5, y = 500, label = "Mean = 2.04 grades", color = "darkred",
           size = 5)+
  labs(x = "Reviewer uncertainty", y = "Count", 
       title = "Distribution of reviewer uncertainty") +
  theme(plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 12, face = "bold"),
        axis.text = element_text(size = 10))

## Plot average uncertainty per reviewer
dpr_reviewer %>%  
  ggplot(aes(avg_uncertainty)) + 
  geom_histogram(binwidth =0.2) 
  
## Repeat for panel (uncertainty only)
prp_shortlisted3 %>% 
  ggplot(aes(uncertainty)) + 
  geom_histogram(binwidth =0.2) 

## Get freq of uncertainty and avg uncertainty for DPR
avg_conf_frequencies <- freq_function(dpr_reviewer, "avg_uncertainty")
conf_frequencies <- freq_function(dpr_funded, "uncertainty")

## Get mean and sd of uncertainty
sd(dpr_funded$uncertainty)
mean(dpr_funded$uncertainty)

## Get freq of confidence range for panel
conf_freq_panel <- freq_function(prp_shortlisted3, "uncertainty")
```

#### Does uncertainty predict vote and ranking?

```{r}
## Plot average uncertainty per proposal by ranking
scatter_funct(dpr_proposal, rank_by_trimmed_mean, proposal_avg_uncertainty)

# Run regression model for vote 
summary(conf_mod <- lm(vote ~ uncertainty, data = dpr_funded))
tab_model(conf_mod, show.se = T,  
          title = "Confidence range as a predictor of vote for DPR")
plot(conf_mod, which = c(1,2,4, 5))

## Get robust SEs
summary(conf_modRob <- lm_robust(vote ~ uncertainty, data = dpr_funded))

# Repeat using trimmed_dpr
summary(conf_trim <- lm(vote ~ uncertainty, data = trimmed_dpr))

# Plot association between confidence range and vote
scatter_funct(dpr_funded, uncertainty, vote)

ggplot(dpr_funded, aes(uncertainty, vote)) +
  geom_jitter(color = "steelblue", size = 1) +
  geom_smooth(method = lm, color = "darkred", linewidth = 0.8) +
  scale_y_continuous(breaks = c(1,2,3,4, 5, 6, 7, 8, 9)) +
  scale_x_continuous(breaks = c(0, 8),
                     labels = c("No uncertainty", "Maximum\nuncertainty")) +
  labs(title = "Association between reviewer uncertainty and score awarded",
       x = "Reviewer uncertainty", y = "Score awarded")

# Run regression model for average proposal uncertainty on ranking
summary(conf_mod2 <- lm(rank_by_trimmed_mean ~ proposal_avg_uncertainty, 
                        data = dpr_proposal))
plot(conf_mod2, which = c(1,2,4,5))

# Try without outlier n.44
subset1 <- dpr_proposal[-44,]
summary(conf_mod2b <- lm(rank_by_trimmed_mean ~ proposal_avg_uncertainty,
                         data = subset1))
plot(conf_mod2b, which = c(1,2,4,5))

## Get robust SEs
summary(conf_mod2Rob <- lm_robust(rank_by_trimmed_mean ~ proposal_avg_uncertainty,
                                  data = dpr_proposal))

## Plot rank by average uncertainty per proposal
scatter1 <- ggplot(dpr_proposal, aes(rank_by_trimmed_mean, proposal_avg_uncertainty)) +
  geom_point(color = "steelblue", size = 2) +
  geom_smooth(method = lm, color = "darkred", linewidth = 0.8) +
  scale_y_continuous(breaks = c(0,1,2,3,4)) +
  scale_x_continuous(breaks = c(1, 20, 40, 60, 80, 100, 120, 140)) +
  labs(title = "Association between proposal ranking and average confidence range of reviewers",
       x = "Rank", y = "Average reviewer confidence range")

#ggsave("scatter1.png", scatter1)
```

## Sentiment of reviewer comments

```{r}
## Join sentiment analysis to dpr_funded
dpr_comments <- left_join(dpr_funded, 
                          sent[, c("sentiment", "sent_confidence", "response_id")],
                          by = "response_id")

## Plot average vote by sentiment
dpr_comments %>%
  ggplot(aes(sentiment, vote)) +
  geom_boxplot(fill = "steelblue" )+
  geom_jitter(color="black", size=0.4, alpha=0.9) +
    labs(x = "Sentiment", y="Score", title = "Scores according to comment sentiment (N=1387)") +
  scale_y_continuous(breaks = c(1, 2,3 ,4 ,5 ,6 ,7 ,8 ,9), limits = c(1,9))

st(dpr_comments, summ = c("mean(x)", "sd(x)", "median(x)", "IQR(x)"), 
   group = "sentiment",
   vars = c("vote", "crit1", "crit2", "crit3", "crit4", "conf_lowest", 
            "conf_highest","uncertainty", "others_avg", "review_time","numeric_dFit",
            "reviewer_range", "working_yrs", "r.adj"),
  digits = 3, out = "return", title = "Statistics by review sentiment")

# Use anova to compare difference in scores by sentiment
summary(aov(vote ~ sentiment, data = dpr_comments))
leveneTest(vote~sentiment, data = dpr_comments) # p<.05 so assumption of equal variances violated

# Conduct Kruskal-Wallis test
kruskal.test(vote ~ sentiment, data = dpr_comments)

# Run bonferroni corrected post-hoc Dunns test (uses FSA package)
dunnTest(vote ~ sentiment, data = dpr_comments, method = "bonferroni")

```

## Potentially strategic scoring

Reminder: **mean_of_mean:** Mean of all reviewer_means

Reviewer Adjustment (**r.adj**): mean_of_mean - reviewer_mean

**mean_shift**: reviewer adjustment + vote

A positive r.adj indicates reviewer was generally low scoring.

```{r}
## Plot the distribution of reviewers' mean scores again, highlighting up to 3 SDs
dpr_reviewer %>%
  ggplot(aes(reviewer_mean)) + 
  geom_histogram(binwidth =0.3) +
  geom_vline(aes(xintercept = mean(reviewer_mean)), color = "orange", 
             linewidth = 1, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(reviewer_mean) - sd(reviewer_mean)), 
             color = "red", linewidth = 0.5, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(reviewer_mean) - 2*(sd(reviewer_mean))),
             color = "blue", linewidth = 0.5, linetype = "dashed") +
    geom_vline(aes(xintercept = mean(reviewer_mean) - 3*(sd(reviewer_mean))), 
               color = "purple", linewidth = 0.5, linetype = "dashed") +
    geom_vline(aes(xintercept = mean(reviewer_mean) + sd(reviewer_mean)), 
               color = "red", linewidth = 0.5, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(reviewer_mean) + 2*(sd(reviewer_mean))), 
             color = "blue", linewidth = 0.5, linetype = "dashed") +
  annotate("text", x= 6.1, y = 42.5, label = "Mean") +
  annotate("text", x= 5,   y = 42.5, label = "- 1 SD") +
  annotate("text", x= 3.9, y = 42.5, label = "- 2 SD") +
  annotate("text", x= 2.8, y = 42.5, label = "- 3 SD") +
  annotate("text", x= 7.2, y = 42.5, label = "+ 1 SD") +
  annotate("text", x= 8.4, y = 42.5, label = "+ 2 SD") +
  scale_y_continuous(expand = c(0,0), limits = c(0, 45)) +
  labs(title = "Distribution of reviewers' mean scores", 
       x = "Mean score per reviewer", y = "Count")

## Look at scoring in relation to the overall average
# Plot the distribution of r.adj
dpr_reviewer %>%
ggplot(aes(r.adj)) + 
  geom_histogram(binwidth =0.2) +
  geom_vline(aes(xintercept = mean(r.adj)), color = "orange", linewidth = 1) +
  labs(subtitle = "Distribution of reviewer adjustment (mean of mean - reviewer mean)\nA positive score indicates generally low scoring ",
       x = "Reviewer adjustment")

## Calculate distance between any score and the mean proposal score (excluding that score)
dpr_behaviour <- dpr_funded %>%
  group_by(proposal_id) %>%
  mutate(mean_without_reviewer = (sum(vote) - vote) / (n() - 1)) %>% 
  ungroup() %>%
  group_by(proposal_id, reviewer_id) %>%
  mutate(diff_from_mean = mean_without_reviewer - vote) %>% # Positive == higher than avg, negative == lower than avg
  ungroup() %>%
  group_by(reviewer_id) %>%
  mutate(avg_difference = mean(diff_from_mean)) %>% # Get the average difference per reviewer
  dplyr::select(proposal_id, reviewer_id, crit1:others_avg, trimmed_mean, rank_by_trimmed_mean,
         mean_without_reviewer, diff_from_mean, avg_difference, mean_of_mean)
  
# Plot the distribution of diff_from_mean
dpr_behaviour %>%
ggplot(aes(diff_from_mean)) + 
  geom_histogram(binwidth =0.2) +
  labs(subtitle = "Distribution of difference from mean (proposal mean without reviewer - reviewer's score)\nA positive score indicates low scoring for this review ", x = "Difference from the mean")

# Plot the distribution of avg_difference
dpr_behaviour %>%
  filter(!duplicated(reviewer_id)) %>%
  ggplot(aes(avg_difference)) + 
  geom_histogram(binwidth = 0.2)+
     geom_vline(aes(xintercept = mean(avg_difference)), color = "orange", 
                linewidth = 1, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(avg_difference) - sd(avg_difference)),
             color = "red", linewidth = 0.5, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(avg_difference) - 2*(sd(avg_difference))), 
             color = "blue", linewidth = 0.5, linetype = "dashed") +
    geom_vline(aes(xintercept = mean(avg_difference) + 3*(sd(avg_difference))), 
               color = "purple", linewidth = 0.5, linetype = "dashed") +
    geom_vline(aes(xintercept = mean(avg_difference) + sd(avg_difference)), 
               color = "red", linewidth = 0.5, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(avg_difference) + 2*(sd(avg_difference))),
             color = "blue", linewidth = 0.5, linetype = "dashed") +
    annotate("text", x= -0.3, y = 33.5, label = "Mean") +
  annotate("text", x= -1.4,   y = 33.5, label = "- 1 SD") +
  annotate("text", x= -2.6, y = 33.5, label = "- 2 SD") +
  annotate("text", x= 3.1, y = 33.5, label = "+ 3 SD") +
  annotate("text", x= 0.8, y = 33.5, label = "+ 1 SD") +
  annotate("text", x= 2, y = 33.5, label = "+ 2 SD") +
  scale_y_continuous(expand = c(0,0), limits = c(0, 35)) +
  labs(subtitle = "Average difference from mean per reviewer (proposal mean without reviewer - reviewer's score)\nA positive score indicates generally low scoring ", x = "Average difference from mean", 
       y ="Count")

```

```{r}
# Basics - counting number of reviews/proposals reviewed 
check <- dpr_funded %>%
  group_by(proposal_id) %>% 
  mutate(no_reviews = length(vote)) %>%
  ungroup() %>%
  group_by(reviewer_id) %>%
  mutate(no_reviewed = length(vote)) %>%
  ungroup()
  
mean(check$no_reviews)
sd(check$no_reviews)

mean(check$no_reviewed)
sd(check$no_reviewed)
```

## Feedback and expectations analysis

```{r}
## Prepare feedback and expectations data
# Create reviewer df from dpr_ funded
dpr_rev <- dpr_funded %>%
  filter(!duplicated(reviewer_id))

## Rename 2_0 and order levels of variables
feedback_df <- feedback_df %>%
  rename(future_dpr = "2_0") %>%
  mutate(politeness = factor(politeness,
                             levels = c("very poor", "poor", "somewhat poor",
                                        "neutral", "somewhat good", 
                                        "good", "very good")),
         helpfulness = factor(helpfulness,
                              levels = c( "very poor", "poor", "somewhat poor",
                                           "neutral", "somewhat good", "good",
                                                    "very good")),
         expertise = factor(expertise, 
                            levels = c( "very poor", "poor", "somewhat poor",
                                          "neutral", "somewhat good", "good",
                                                    "very good")),
         future_dpr = factor(future_dpr, 
                             levels = c( "very negative",  "somewhat negative",
                                         "neutral", "somewhat positive",
                                                    "very positive"))) 
                         
## Add demographic information to feedback_df from dpr_rev
feedback <- left_join(feedback_df,
                      dpr_rev[, c("reviewer_id", "gender", "working_yrs",
                                  "working_quant", "ecr","first_gen")],
                      by = c("apl_id"= "reviewer_id"))

## Add demographic information to expect_df from dpr_rev
expect <- left_join(expect_df, dpr_rev[, c("reviewer_id", "gender", "working_yrs",
                                           "working_quant", "ecr", "first_gen")],
                    by = c("applicant_id"= "reviewer_id"))
```

**Plotting frequency of feedback question responses**

```{r}
# Count responses with text comment - won't work in repo as comments removed for anonymisation
#feedback_df %>%
#  filter(feedback != "none") %>%
#  count()

#Count responses which don't report politeness/helpfulness/expertise/future dpr
feedback_df %>%
  filter(is.na(politeness)) %>%
  count()

feedback_df %>%
  filter(is.na(helpfulness)) %>%
  count()

feedback_df %>%
  filter(is.na(expertise)) %>%
  count()

feedback_df %>%
  filter(is.na(future_dpr)) %>%
  count()

## Plot frequency of feedback variables
## Plot freq of politeness
feedback %>%
  filter(!is.na(politeness)) %>%
  group_by(politeness) %>%
  summarise(count = n()) %>%
  ggplot() +
  geom_col(aes(count, politeness), fill = "#4292C6", color = "darkgrey", width = 0.8) +
  scale_x_continuous(limits = c(0, 55), expand = c(0,0)) +
  scale_y_discrete(labels = c("Very poor", "Poor", "Somewhat poor","Neutral", "Somewhat good",
                   "Good","Very good")) +
  geom_hline(yintercept = "neutral", linetype = "dashed") +
  labs(title =  "Politeness of reviews",
       x = "Number of responses", y = NULL)+
  theme(plot.title = element_text(size = 20),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18))  

## Plot freq of helpfulness
feedback %>%
  filter(!is.na(helpfulness)) %>%
  group_by(helpfulness) %>%
  summarise(count = n()) %>%
ggplot() +
  geom_col(aes(count, helpfulness), fill = "#4292C6", color = "darkgrey", width = 0.8) +
  scale_x_continuous(limits = c(0, 55), expand = c(0,0)) +
   scale_y_discrete(labels = c("Very poor", "Poor", "Somewhat poor","Neutral", "Somewhat good",
                   "Good","Very good")) +
  geom_hline(yintercept = "neutral", linetype = "dashed") +
  labs(title =  "Helpfulness of reviews",
       x = "Number of responses", y = NULL) +
  theme(plot.title = element_text(size = 20),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18)) 

## Plot freq of expertise
feedback %>%
  filter(!is.na(expertise)) %>%
  group_by(expertise) %>%
  summarise(count = n()) %>%
  ggplot() +
  geom_col(aes(count, expertise), fill = "#4292C6", color = "darkgrey", width = 0.8) +
  scale_y_discrete(labels = c("Very poor", "Poor", "Somewhat poor","Neutral", "Somewhat good",
                   "Good","Very good")) +
  scale_x_continuous(limits = c(0, 55), expand = c(0,0)) +
  geom_hline(yintercept = "neutral", linetype = "dashed") +
  labs(title =  "Expertise of reviews",
       x = "Number of responses", y = NULL)+
  theme(plot.title = element_text(size = 20),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18)) 

## Plot freq of attitude to future DPR
feedback %>%
  filter(!is.na(future_dpr)) %>%
  group_by(future_dpr) %>%
  summarise(count = n()) %>%
  ggplot() +
  geom_col(aes(count, future_dpr), fill = "#4292C6", color = "darkgrey", width = 0.8) +
  scale_x_continuous(limits = c(0, 55), expand = c(0,0)) +
  geom_hline(yintercept = "neutral", linetype = "dashed") +
  scale_y_discrete(labels = c("Very negative", "Somewhat\nnegative", "Neutral",
                              "Somewhat\npositive", "Very positive")) +
  labs(title =  "Attitude towards future DPR", x = "Number of responses", y = NULL)  +
  theme(plot.title = element_text(size = 20),
        axis.title = element_text(size = 18),
        axis.text = element_text(size = 18)) 


```

### Responses according to career stage

```{r}
## Define frequency function 3
freq_function3<- function(df, var) {
  df %>%
    filter(!is.na({{var}})) %>%
    group_by({{var}}) %>%   
    summarise(count = n()) %>%
    ungroup() %>%
    mutate(percentage = round((count/sum(count) * 100), 2)) %>%
    arrange(desc({{var}})) %>%
    mutate(cumulative_percentage = round(cumsum(percentage), 2))
}

## Define frequency function 4 grouping by 2 variables 
freq_function4<- function(df, var1, var2) {
  df %>%
    filter(!is.na({{var1}})) %>%
    group_by({{var1}}, {{var2}}) %>%
    summarise(count = n(), .groups = "drop") %>%
    group_by({{var2}}) %>%
    mutate(percentage = round((count/sum(count) * 100), 2)) %>%
    arrange(desc({{var1}})) %>%
    mutate(cumulative_percentage = round(cumsum(percentage), 2)) 
   }
#Frequency of all variables
poli_freq <- freq_function3(feedback, politeness)
help_freq<- freq_function3(feedback, helpfulness)
expertise_freq <- freq_function3(feedback, expertise)

# Frequency of feedback according to characteristics
wq_freq<- freq_function3(feedback, working_quant) 
gen_freq <- freq_function3(feedback, gender) 
first_freq <- freq_function3(feedback, first_gen)
  
# Frequency of responses to future attitudes according to characteristics
wq_freq2<- freq_function4(feedback, future_dpr, working_quant) 
gen_freq2 <- freq_function4(feedback, future_dpr,gender) 
first_freq2 <- freq_function4(feedback,future_dpr, first_gen)

## Plot freq of attitude to future DPR by working years quantiles
wq_freq2 %>%
  mutate(working_quant = factor(working_quant, levels = c(1,2,3,4)),
         future_dpr = factor(future_dpr, 
                             levels = c("very positive", "somewhat positive", 
                                        "neutral", "somewhat negative", 
                                        "very negative"))) %>%
  ggplot() +
  geom_col(aes(y= percentage, x = working_quant , fill = future_dpr), 
           position = "dodge", color = "darkgrey", width = 0.7) +
scale_x_discrete(labels = c("0-8 yrs\n(n=34)", "9-12 yrs\n(n=23)", "13-19 yrs\n(n=33)",
                            "20-52 yrs\n(n=33)")) +
  scale_fill_manual(values = c("#d4e2eb", "#9ECAE1", "#4292C6", "#2171B5",  "#08306B"),
                    labels = c("Very positive", "Somewhat positive", "Neutral",
                               "Somewhat negative", "Very negative"), 
                    name = "Attitude to future DPR") +
  labs(title =  "Attitude towards participating in a future call using DPR according\nto years spent working in research", y = "Percentage", 
       x = "\nYears working\nin research") +
  theme(axis.text.y = element_text(size = 10), legend.position = "right")

## Get mean and median working years
working_stats <- feedback %>%
  summarize(mean_working = mean(working_yrs, na.rm = T),
            sd_working = sd(working_yrs, na.rm = T),
            med_working = median(working_yrs, na.rm = T),
            IQR_working = IQR(working_yrs, na.rm = T))           
working_stats

```

```{r}
## Look at constructiveness / non-constructiveness of reviews
feedbackb <- feedback %>%
  mutate(constructive = rowSums(dplyr::select(., "1_0": "1_9") == "constructive"),
         non_constructive = rowSums(dplyr::select(., "1_0": "1_9") == "non-constructive"))

## Add a proposal_id column to feedbackb 
feedbackb <- feedbackb %>%
    mutate(proposal_id = sub("_.*", "", apl_id),# Take start of apl_id and create proposal_id column
           proposal_id = as.numeric(proposal_id))  

## Get frequency of constructive and non-constructive comments
freq_function3(feedbackb, constructive) 
freq_function3(feedbackb, non_constructive) 

```

### Understanding consistency in perceptions of feedback

```{r}
## Pivot data so feedback per item from applicants from the same proposal is on
## the same row
feedback2 <- feedback %>%
  dplyr::select(apl_id, "1_0": "1_9") %>%
  mutate(proposal_id = sub("_.*", "", apl_id),
         apl_suffix = sub(".*_", "app", apl_id)) %>% # Take start of apl_id and create proposal_id column
  group_by(proposal_id) %>%  
  pivot_longer(cols = c("1_0": "1_9"),
               names_to = "Item", values_to = "value") %>%
  mutate(value = factor(value, levels = c("constructive", "somewhat constructive", 
                                          "neutral", "somewhat non-constructive", 
                                          "non-constructive",
                                             "none", "not applicable"))) %>%
  ungroup()

## Get frequency of constructiveness feedback without none or not applicable responses
feedback2b <- feedback2 %>%
  filter(value != "none" & value  != "not applicable")

## Get frequency of constructiveness responses
constructiveness <- freq_function3(feedback2b, value) 

```

```{r}
## Identify proposals with feedback from 2+ applicants and recode ratings 
feedback3 <- feedback2 %>%
  dplyr::select(-apl_id) %>%
  pivot_wider(names_from = apl_suffix, values_from = value, 
              values_fn = list(value = ~ first(.))) %>%
  filter((!is.na(app1) & !is.na(app2)) | (!is.na(app1) & !is.na(app3)) | # only include proposals with 2+ applicant feedback
           (!is.na(app2) & !is.na(app3))) %>%

## Recode ratings to change letters to numbers 
    mutate(across(c(app1:app3), 
                ~ dplyr::recode(., 'constructive' = 5, 'somewhat constructive' = 4,
                                'neutral' = 3, 'somewhat non-constructive' = 2, 
                                'non-constructive' = 1))) %>% #none or not-applicable treated as NA
  ## Coalesce columns app1 and app2 with values from app3 
  mutate(app1 = coalesce(app1, app3),
         app2 = coalesce(app2, app3)) %>%
  dplyr::select(-app3) %>% # note that app3 will be in app1 or app2 
  ungroup()

freq_function3(feedback3, proposal_id)

## Calculate weighted kappa to look at agreement re feedback
feedback_kappa <- feedback3 %>%
  group_by(proposal_id) %>%
  summarise(kappa = kappa2(pick(app1, app2), weight = "squared")$value,
            z_stat = kappa2(pick(app1, app2), weight = "squared")$statistic,
            p_value = kappa2(pick(app1, app2), weight = "squared")$p.value) %>%
  ungroup()

## Calculate the average weighted kappa
feedback_kappa %>%
  filter(!is.na(p_value)) %>% # omits proposals who got NaN due to one reviewer providing all same responses
  summarise(mean(kappa))

## Plot the distribution of weighted kappa
feedback_kappa %>%
  filter(!is.na(p_value)) %>%
  ggplot(aes(kappa)) + 
  geom_histogram(binwidth = 0.10, fill = "#2171B5") +
  scale_x_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5,
                                0.6,  0.7, 0.8, 0.9, 1.0),
                     labels = c("-0.3\n", "-0.2\n\nPoor", "-0.1\n", "0\n",
                                "0.1\n\nSlight", "0.2\n", "0.3\n\nFair", "0.4\n",
                                "0.5\n\nModerate", "0.6\n", "0.7\n\nSubstantial", 
                                "0.8\n", "0.9\n\nAlmost\nperfect", "1\n")) +
  labs(title = "Distribution of agreement between co-applicants regarding the constructiveness of\nreview comments", subtitle ="(For proposals where 2+ applicants provided feedback, n =41)", x = "\nWeighted kappa\n(Agreement)",
       y = "Frequency")

## Does disciplinary overlap between applicants predict agreement?
 # Make proposal_id same class for joining 
dpr_funded <- dpr_funded %>%
  mutate(proposal_id = as.factor(proposal_id)) 
## Join matching/overlap columns to feedback_kappa
feedback_kappa2 <- left_join(feedback_kappa, 
                             dpr_funded[, c("proposal_id", "matching_rev1_2", 
                                            "matching_rev2_3", "matching_rev1_3")],
                             by = "proposal_id")

## Coalesce matching_ variables (no proposal had reports from more than 2)
feedback_kappa2 <- feedback_kappa2 %>%
  filter(!duplicated(proposal_id)) %>%
    mutate(matching = coalesce(matching_rev1_2, matching_rev1_3),
         matching2 = coalesce(matching, matching_rev2_3),
         matching2 = factor(matching2)) %>%
  ungroup()

# Regress kappa on matching2
summary(lm(kappa ~ matching2, feedback_kappa2))

```

```{r}
## Join trimmed mean and original ranking to feedbackb
feedbackb <- feedbackb %>%
  mutate(proposal_id = factor(proposal_id))
dpr_proposal <- dpr_proposal %>%
  mutate(proposal_id = factor(proposal_id))
         
feedback_rank <- left_join(feedbackb, 
                           dpr_proposal[, c("proposal_id", "trimmed_mean",
                                            "rank_by_trimmed_mean", "funded_panel", 
                                            "funded_DPR", "funded_any")], 
                           by = "proposal_id")

## Recode politeness, helpfulness, expertise and future dpr to numbers for regression
feedback_rank2 <- feedback_rank %>%
  mutate(proposal_id = factor(proposal_id),
         funded_any = factor(funded_any)) %>%
    mutate(across(c(politeness:expertise), 
                ~ dplyr::recode(., 'very good' = 7, # Lower is better to match expectations scales
                         'good' = 6, 'somewhat good' = 5, 'neutral' = 4, 
                         'somewhat bad' = 3, 'bad' = 2, 'very bad' = 1))) %>%
  mutate(future_dpr = dplyr::recode(future_dpr, 'very positive' = 5, 
                                    'somewhat positive' = 4,'neutral' = 3,
                                    'somewhat negative' =2, 'very negative' =1))

## Count no. proposals
feedback_rank2 %>%
  filter(!duplicated(proposal_id)) %>%
  count() # 84

## Check no.reviewers who were not funded
funded_rev <- feedback_rank2 %>%
  filter(funded_any == 1) 
not_funded = nrow(feedback_rank2) - nrow(funded_rev)

## Get distribution of trimmed_mean for proposals which received feedback
overall_mean <- mean(dpr_funded$trimmed_mean) # Get overall trimmed_mean (across all proposals)
feedback_rank%>%
  filter(!duplicated(proposal_id)) %>%
  ggplot(aes(trimmed_mean)) + 
  geom_histogram(binwidth =0.3, fill = "grey") +
  geom_vline(xintercept = overall_mean, color = "darkorange", linewidth = 1 , 
             linetype = "dashed") +
  labs(x = "Trimmed mean", title = "Distribution of trimmed mean of proposals from which one or more applicants provided\nfeedback", y = "Count") +
  annotate("text", x= 5.3, y = 18, label = "Mean across\nall 140 proposals",
           color = "darkorange")



```

###Reviewer expectations

```{r}
## Rename expectations variables
expect <- expect %>%
  rename(apl_id = 'applicant_id',
         suitable = "DPR suitable to review",
         app_reviewer= "DPR identifes appropriate reviewers",
         fair = "Proposal will receive fair evaluation",
         best = "Process will select best research",
         adventurous = "DPR selects more adventuorous proposals",
         similar = "DPR selects similar set of porposals")


## Recode expectations variables to match direction of feedback
expect2 <- expect %>%
    mutate(across(c(suitable, app_reviewer, fair, best , adventurous, similar), 
                ~ dplyr::recode(.,  '1' = 7, 
                         '2' = 6, '3' = 5, '4' = 4, 
                         '5' = 3, '6' = 2, '7' = 1)))

## Remove 2 X coapplicants who did not provide expectations data
expect2 <- expect2 %>%
  filter(!is.na(suitable) | !is.na(app_reviewer) |!is.na(fair) | !is.na(best)|
           !is.na(adventurous) | !is.na(similar))

## Get frequencies for all variables
suitability_freq <- freq_function3(expect2, suitable)
app_reviewer_freq <- freq_function3(expect2, app_reviewer)
fair_freq <- freq_function3(expect2, fair)
best_freq <- freq_function3(expect2, best)
adv_freq <- freq_function3(expect2, adventurous)
similar_freq <- freq_function3(expect2, similar)

## Define freq to plot expectations variables
expect_funct <- function(df, var, fill){
  df %>%
  mutate(var = factor({{var}})) %>%
  ggplot() +
  geom_col(aes(count, var), fill = fill, color = "darkgrey", width = 0.8) +
  scale_x_continuous(limits = c(0, 165), expand = c(0,0)) +
  scale_y_discrete(labels = c("Strongly disagree", "Disagree", "Slightly disagree",
                              "Neither agree\nor disagree", "Slightly agree",
                   "Agree","Strongly agree")) +
  geom_hline(yintercept = 4, linetype = "dashed") +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14))  
}

## Plot suitability
suitability_plt <- expect_funct(suitability_freq, suitable, "#9ECAE1")
suitability_plt + labs(title =  "This process is suitable to review my proposal",
       x = "Number of responses", y =NULL)

## Plot app_reviewer
app_rev_plt <- expect_funct(app_reviewer_freq, app_reviewer, "#9ECAE1")
app_rev_plt + labs(title =  "This process will identify appropriate reviewers", 
                   x = "Number of responses", y =NULL)

## Plot fair
fair_plt <- expect_funct(fair_freq, fair, "#9ECAE1")
fair_plt + labs(title =  "My proposal will receive a fair evaluation under\nthis process", 
                x = "Number of responses", y =NULL) 

## Plot best
best_plt <- expect_funct(best_freq, best, "#9ECAE1")
best_plt + labs(title =  "Overall, I trust this process to select the best\nresearch to fund", y =NULL, x = "Number of responses")

## Plot adventurous
adv_plt <- expect_funct(adv_freq, adventurous, "#2171B5")
adv_plt +  labs(title =  "I expect this process to select more adventurous\nproposals than conventional panel review", y =NULL, x = "Number of responses") 

## Plot similar without using expect_function to keep all levels 
expect2 %>%
  filter(!is.na(similar)) %>%
  mutate(similar = factor(similar, levels = c(1,2,3,4,5,6,7))) %>%
  group_by(similar, .drop = FALSE) %>%
  summarise(count = n()) %>%
  ggplot() +
  geom_col(aes(count,similar), fill = "#2171B5", color = "darkgrey", width = 0.8) +
  scale_x_continuous(limits = c(0, 165), expand = c(0,0)) +
  scale_y_discrete(labels = c("Strongly disagree", "Disagree", "Slightly disagree",
                              "Neither agree\nor disagree", "Slightly agree",
                   "Agree","Strongly agree")) +
  geom_hline(yintercept = 4, linetype = "dashed") +
  labs(title =  "I expect the process to select a similar set of\nproposals to panel review", y =NULL, x = "Number of responses")+
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 14),
        axis.text = element_text(size = 14)) 
```

## Alternative plotting for expectations (figure3) (used in journal article) 
```{r}
## Pivot expect2 longer and calculate percentages of responses to each item
long_expect <- expect2 %>%
  dplyr::select(suitable:similar) %>%
  pivot_longer(cols = everything()) %>%
  mutate(value = factor(value, levels = c(1,2,3,4,5,6,7)),
         name = factor(name, levels = c("adventurous", "similar", "suitable", "app_reviewer", "fair", "best"))) %>%
  group_by(name,value) %>%
  drop_na() %>%
    summarise(count = n()) %>%
    mutate(percentage = round((count/sum(count) * 100), 2))

# Load RColorBrewer for color palette
library(RColorBrewer)
# Plot expectations
ggplot(long_expect, aes(x = percentage, y = name)) +
  geom_col(aes(fill = value), width = 0.6) +
  scale_y_discrete(labels = c( "I expect this process to select more adventurous\nproposals than conventional panel review", "I expect this process to select a similar\nset of proposals to panel review","This process is suitable to review my proposal", "This process will identify appropriate reviewers", "My proposal will receive a fair evaluation\nunder this process", "Overall I trust this process to select\nthe best research to fund")) +
  scale_fill_brewer(palette = "RdBu", labels = c("Strongly disagree", "Disagree", "Slightly disagree", "Neither agree\nor disagree", "Slightly agree", "Agree","Strongly agree"),
                    name = NULL, guide = "legend" ) +
  labs(y = NULL, x = "Percentage of applicants") +
  guides(fill = guide_legend(reverse = TRUE)) +
  theme(axis.text.y = element_text(size = 10), 
        legend.text = element_text(size = 10), legend.position = "right")
```

```{r}
## Join expectation and feedback variables
pre_post <- left_join(expect2, feedback_rank2, by = "apl_id")

## Add pre_expectations variable for average score across suitability, 
##app_reviewer, fair and best
pre_post2 <- pre_post %>%
  mutate(mean_expect = (suitable + app_reviewer + fair + best + adventurous)/5, 
         na.rm = T, # Average expectations
         general_expect = case_when(mean_expect >4 ~ "positive", # Classify as positive, negative or neutral
                                    mean_expect <4 ~ "negative",
                                    mean_expect ==4 ~ "neutral"),   

## add post_expectations variable for average score across helpfulness, expertise and politeness
        mean_feedback = (helpfulness + expertise + politeness)/3, na.rm = T,
        general_feedback = case_when(mean_feedback >4 ~ "positive",
                                     mean_expect <4 ~ "negtive",
                                     mean_expect == 4 ~ "neutral"))

## Get descriptives
feedback_descriptives <- st(pre_post2, 
                            summ = c("mean(x)", "sd(x)", "se(x)", "median(x)",
                                     "IQR(x)", "range(x)"), 
                            vars = c("suitable","app_reviewer", "fair", "best", 
                                     "adventurous", "similar", "mean_expect", 
                                     "general_expect", "trimmed_mean", 
                                     "rank_by_trimmed_mean", "funded_panel", 
                                     "funded_DPR", "funded_any", "constructive", 
                                     "non_constructive", "helpfulness", "politeness", 
                                     "expertise", "future_dpr", "mean_feedback",
                                     "general_feedback"), digits = 3, out = "return")

## Get descriptives according to whether funded or not
feedback_byFunding <- st(pre_post2, summ = c("mean(x)", "sd(x)", "se(x)", 
                                             "median(x)", "IQR(x)", "range(x)"),
    vars = c("suitable", "app_reviewer", "fair", "best", "adventurous", "similar",
             "mean_expect", "general_expect", "trimmed_mean", "rank_by_trimmed_mean", 
             "constructive", "non_constructive", "helpfulness", "politeness",
             "expertise", "future_dpr", "mean_feedback", "general_feedback"), 
    group = "funded_any", digits = 3, out = "return")

## Get future DPR by funding status
feedback_rank2 %>%
  filter(!is.na(future_dpr)) %>%
  group_by(future_dpr, funded_any) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(funded_any) %>%
  mutate(percentage = round((count / sum(count) * 100), 2)) %>%
  ungroup() %>%
  pivot_wider(names_from = funded_any, values_from = c(percentage, count))

freq_function4(feedback_rank2, future_dpr, funded_any)
summary(feedback_rank2$future_dpr) 


```

